{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ea687499",
   "metadata": {},
   "source": [
    "# GeoBench Jupyter Notebook Benchmarking Example\n",
    "\n",
    "This notebook demonstrates how to use GeoBench to benchmark code execution in Jupyter notebooks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "80f60c4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the JupyterBenchmark class and benchmark decorator\n",
    "from geobench.jupyter import JupyterBenchmark, benchmark\n",
    "import time\n",
    "import math\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5d80188",
   "metadata": {},
   "source": [
    "## Method 1: Using the JupyterBenchmark Class\n",
    "\n",
    "You can use the `JupyterBenchmark` class to manually start and finish benchmarking around code execution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "a9a05df7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a benchmark instance\n",
    "bench = JupyterBenchmark(\n",
    "    name=\"prime-number-count\",\n",
    "    outdir=\"jupyter-benchmark-results\",\n",
    "    run_monitor=2.0,  # Monitor for 2 seconds before and after execution\n",
    "    clean=True  # Clean output directory if it exists\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "07ccbb53",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to benchmark\n",
    "def count_primes(n):\n",
    "    count = 0\n",
    "    for i in range(2, n):\n",
    "        if all(i % j != 0 for j in range(2, int(math.sqrt(i)) + 1)):\n",
    "            count += 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2dd47375",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark: count-primes-1000k\n",
      "Storing system information.\n",
      "Clearing system caches.\n",
      "Waiting 2.0 s before the run.\n",
      "Baseline monitoring for 2.0 s.\n",
      "Process monitoring started.\n",
      "Found 78498 prime numbers.\n",
      "Process monitoring stopped.\n",
      "Waiting 2.0 s after the run.\n",
      "Endline monitoring for 2.0 s.\n",
      "Benchmark completed in 5.16 s.\n"
     ]
    }
   ],
   "source": [
    "# Start benchmarking\n",
    "bench.start(\"count-primes-1000k\")\n",
    "\n",
    "# Execute code to benchmark\n",
    "try:\n",
    "    result = count_primes(1000000)\n",
    "    print(f\"Found {result} prime numbers.\")\n",
    "    success = True\n",
    "except Exception as e:\n",
    "    print(f\"Error: {e}\")\n",
    "    success = False\n",
    "\n",
    "# Finish benchmarking\n",
    "summary = bench.finish(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5ea8e50b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Report generated at /Users/bhawiyuga/project/geobench-projects/geobench/examples/jupyter-benchmark-results/report.html\n",
      "Report generated at: /Users/bhawiyuga/project/geobench-projects/geobench/examples/jupyter-benchmark-results/report.html\n"
     ]
    }
   ],
   "source": [
    "# Generate HTML report\n",
    "report_path = bench.generate_report()\n",
    "print(f\"Report generated at: {report_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b3ed7df",
   "metadata": {},
   "source": [
    "## Method 2: Using the Benchmark Decorator\n",
    "\n",
    "For a simpler approach, you can use the `@benchmark` decorator."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "433b9553",
   "metadata": {},
   "outputs": [],
   "source": [
    "from count_primes import count_primes\n",
    "\n",
    "# Define a function with the benchmark decorator\n",
    "@benchmark(name=\"parallel-prime-count\", outdir=\"jupyter-benchmark-decorator\", clean=True)\n",
    "def parallel_count_primes(n, cores=4):\n",
    "    import multiprocessing\n",
    "    \n",
    "    with multiprocessing.Pool(cores) as pool:\n",
    "        results = pool.map(count_primes, [n]*cores)\n",
    "\n",
    "    return sum(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "32e9b010",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting benchmark: parallel_count_primes\n",
      "Storing system information.\n",
      "Clearing system caches.\n",
      "Waiting 2.0 s before the run.\n",
      "Baseline monitoring for 2.0 s.\n",
      "Process monitoring started.\n",
      "Executing the function with process monitoring.\n",
      "Process monitoring stopped.\n",
      "Waiting 2.0 s after the run.\n",
      "Endline monitoring for 2.0 s.\n",
      "Benchmark completed in 5.18 s.\n",
      "Report generated at /Users/bhawiyuga/project/geobench-projects/geobench/examples/jupyter-benchmark-decorator/report.html\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "313992"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "parallel_count_primes(1_000_000, cores=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6a178c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "geobench",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
